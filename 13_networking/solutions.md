# Networking and Web - Solutions

## Solution 1: Weather API Client

```python
import requests

def get_weather(city, api_key):
    """Get current weather for a city (using OpenWeatherMap)."""
    base_url = "http://api.openweathermap.org/data/2.5/weather"
    params = {
        'q': city,
        'appid': api_key,
        'units': 'metric'
    }

    try:
        response = requests.get(base_url, params=params, timeout=5)
        response.raise_for_status()
        data = response.json()

        return {
            'temperature': data['main']['temp'],
            'humidity': data['main']['humidity'],
            'description': data['weather'][0]['description']
        }
    except requests.exceptions.RequestException as e:
        print(f"Error: {e}")
        return None

# Test
weather = get_weather('London', 'your_api_key')
if weather:
    print(f"Temperature: {weather['temperature']}Â°C")
    print(f"Humidity: {weather['humidity']}%")
    print(f"Conditions: {weather['description']}")
```

## Solution 2: URL Shortener API

```python
import requests

class URLShortener:
    BASE_URL = 'https://api-ssl.bitly.com/v4'

    def __init__(self, access_token):
        self.headers = {'Authorization': f'Bearer {access_token}'}

    def shorten(self, url):
        """Shorten a URL using Bitly API."""
        endpoint = f'{self.BASE_URL}/shorten'
        data = {'long_url': url}

        response = requests.post(endpoint, json=data, headers=self.headers)
        response.raise_for_status()
        return response.json()['link']

    def expand(self, short_url):
        """Expand a shortened URL."""
        # Extract bitlink
        bitlink = short_url.replace('https://', '').replace('http://', '')
        endpoint = f'{self.BASE_URL}/bitlinks/{bitlink}'

        response = requests.get(endpoint, headers=self.headers)
        response.raise_for_status()
        return response.json()['long_url']

# Test
shortener = URLShortener('your_token')
short = shortener.shorten('https://www.python.org')
print(f"Short URL: {short}")
long = shortener.expand(short)
print(f"Original: {long}")
```

## Solution 3: Batch API Requests

```python
import requests
import time

def batch_requests(urls, batch_size=5, delay=1):
    """Make requests in batches."""
    results = []

    for i in range(0, len(urls), batch_size):
        batch = urls[i:i+batch_size]

        for url in batch:
            try:
                response = requests.get(url, timeout=5)
                results.append((url, response))
            except requests.exceptions.RequestException as e:
                print(f"Error fetching {url}: {e}")
                results.append((url, None))

        # Wait between batches (except last)
        if i + batch_size < len(urls):
            print(f"Processed batch, waiting {delay}s...")
            time.sleep(delay)

    return results

# Test
urls = [f'https://httpbin.org/delay/{i%3}' for i in range(12)]
results = batch_requests(urls, batch_size=3, delay=2)
print(f"Processed {len(results)} requests")
```

## Solution 4: JSON Data Validator

```python
import requests

def validate_response(response, schema):
    """Validate JSON response against schema."""
    if response.status_code != 200:
        return False

    try:
        data = response.json()
    except ValueError:
        return False

    # Check required fields
    for field, field_type in schema.items():
        if field not in data:
            return False
        if not isinstance(data[field], field_type):
            return False

    return True

# Test
response = requests.get('https://api.github.com/users/octocat')
schema = {
    'login': str,
    'id': int,
    'name': str,
    'public_repos': int
}

if validate_response(response, schema):
    print("Response is valid")
else:
    print("Response validation failed")
```

## Solution 5: API Response Cache

```python
import requests
import time
from collections import OrderedDict

class APICache:
    def __init__(self, ttl=300, max_size=100):
        """Initialize cache with TTL in seconds."""
        self.ttl = ttl
        self.max_size = max_size
        self.cache = OrderedDict()  # {url: (response, timestamp)}

    def get(self, url):
        """Get cached response or None."""
        if url in self.cache:
            response, timestamp = self.cache[url]
            if time.time() - timestamp < self.ttl:
                self.cache.move_to_end(url)  # LRU
                return response
            else:
                del self.cache[url]  # Expired
        return None

    def set(self, url, response):
        """Cache response."""
        self.cache[url] = (response, time.time())

        # Evict oldest if over size
        if len(self.cache) > self.max_size:
            self.cache.popitem(last=False)

# Test
cache = APICache(ttl=5)

url = 'https://api.github.com/users/octocat'

# First request (not cached)
cached = cache.get(url)
if not cached:
    print("Cache miss, fetching...")
    response = requests.get(url)
    cache.set(url, response.json())
    data = response.json()
else:
    data = cached

print(f"User: {data['name']}")

# Second request (cached)
time.sleep(1)
cached = cache.get(url)
if cached:
    print("Cache hit!")
```

## Solution 6-10: Abbreviated Solutions

Due to length constraints, here are the key implementations:

**Solution 6: Download Progress Bar**
```python
import requests
from tqdm import tqdm  # pip install tqdm

def download_with_progress(url, filename):
    response = requests.get(url, stream=True)
    total = int(response.headers.get('content-length', 0))

    with open(filename, 'wb') as f, tqdm(total=total, unit='B', unit_scale=True) as pbar:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)
            pbar.update(len(chunk))
```

**Solution 7: Rate Limited Client**
```python
import time

class RateLimitedClient:
    def __init__(self, calls_per_minute=60):
        self.min_interval = 60.0 / calls_per_minute
        self.last_call = 0

    def request(self, url):
        elapsed = time.time() - self.last_call
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)

        self.last_call = time.time()
        return requests.get(url)
```

**Solution 8: Parallel Downloads**
```python
from concurrent.futures import ThreadPoolExecutor
import requests
import os

def download_file(url, output_dir):
    filename = os.path.join(output_dir, url.split('/')[-1])
    response = requests.get(url)
    with open(filename, 'wb') as f:
        f.write(response.content)
    return filename

def parallel_download(urls, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(download_file, url, output_dir) for url in urls]
        return [f.result() for f in futures]
```

**Solution 9: REST API Wrapper**
```python
import requests

class APIClient:
    def __init__(self, base_url, api_key):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({'Authorization': f'Bearer {api_key}'})

    def get(self, endpoint, params=None):
        return self.session.get(f'{self.base_url}/{endpoint}', params=params).json()

    def post(self, endpoint, data=None):
        return self.session.post(f'{self.base_url}/{endpoint}', json=data).json()

    def put(self, endpoint, data=None):
        return self.session.put(f'{self.base_url}/{endpoint}', json=data).json()

    def delete(self, endpoint):
        return self.session.delete(f'{self.base_url}/{endpoint}').json()
```
